{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Watching the Watchmen: Finding Spyplanes with Autoencoder Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the weirder datasets on Kaggle is the 'spyplanes' dataset. It consists of a bunch of flight data from 2015 and was originally used as part of a Buzzfeed News article where the author reckoned he could detect spyplanes apart from other aircraft using machine learning. His analysis worked by training a model on flight data for known FBI and US Security Services aircraft and then looking for other aircraft with similar behaviour.\n",
    "\n",
    "I think there is another approach to this problem.\n",
    "\n",
    "Surveillance-aircraft are, by definition, supposed to be difficult to identify. By comparing aircraft behaviour with that of known government aircraft, we risk missing many other planes which may behave drastically differently to those we manually identified. Rather than training a model on a known dataset (aka supervised learning), I decided it is more appropriate to scour the whole dataset of flight data for unusual behaviour (aka unsupervised learning). In other words, I'm going to look for aircraft which look 'weird'...otherwise known as outliers. To do this I'm going to use an autoencoder neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Full details of the dataset used are available at https://www.kaggle.com/jboysen/spy-plane-finder\n",
    "    \n",
    "It consists of modified flight data from 2015. As this project was more about finding an excuse to use an autoencoder, I chose a pretty convenient dataset which has already been engineered for features. Were I using raw flight data, I'd probably have performed the feature engineering differently, but that's not interesting now.\n",
    "\n",
    "Each entry includes an aircraft with a number of distance/time based metrics, to give an idea of the typical flight pattern. Also included is the aircraft type and transponder info and number of observations/flights seen over the period.\n",
    "\n",
    "The original author also included a small directory of aircraft which he believed were spy aircraft (as they were operated by the FBI or US Defence Department) and ones which he believed were not. In total he identified 97 probably spy aircraft and 500 non-spy aircraft. He used this 'known' dataset to train up his model. I am going to used it for model validation, but not in the normal sense. Remember, we don not know that all of the 97 aircraft are actually spyplanes and any of the 500 ones could be too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than training the model on a dataset and then testing it with known results, a kind of hybrid approach was used. A model was trained on a subset of the main dataset and then tested on the table of known aircraft. Since we do not know how many of the labels on the 'known' aircraft are correct, the performance was only really an estimate. The model was then applied to the rest of the main dataset to give candidate spyplanes. Normally we'd just run a model on the raw dataset and let it tell us which entries were the outliers, but in this case we have the luxury of some 'sort of' labelled data.\n",
    "\n",
    "First the necessary libraries are installed and the main dataset imported. A second table called 'labelled_data' is created. This contains the 'known' spyplanes and non-spyplanes and will be used to test the model. These aircraft are removed from the main dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras import regularizers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#import dataset\n",
    "df=pd.read_csv(\"planes_features.csv\")\n",
    "\n",
    "#convert type column to integer (there are probably better approaches to use here)\n",
    "df['type']=df['type'].astype('category').cat.codes\n",
    "\n",
    "#import labelled aircraft\n",
    "test_ident = pd.read_csv(\"train.csv\")\n",
    "\n",
    "#use the labelled data as a train/test set (note the different context of testing to normal for this model)\n",
    "labelled_data=df[df['adshex'].isin(test_ident['adshex'])]\n",
    "labelled_data=pd.merge(labelled_data,test_ident,on=['adshex','adshex'])\n",
    "labelled_data['type']=labelled_data['type'].astype('category').cat.codes\n",
    "labelled_data=labelled_data.drop(['adshex'],axis=1)\n",
    "\n",
    "df=df[~df['adshex'].isin(test_ident['adshex'])]\n",
    "df_adshex=df['adshex']\n",
    "df=df.drop(['adshex'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10% of the aircraft from the main dataset are then removed and used to build a training set. Note, the data was scaled between 0-1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use 10% of the input data as a training set\n",
    "df,train_set=train_test_split(df,test_size=0.1,random_state=57)\n",
    "\n",
    "#also consider adding some of the actual data to the train/test set to improve the size\n",
    "test_set = labelled_data\n",
    "\n",
    "#save test set labels for later\n",
    "test_set_labels=test_set['class']\n",
    "#get number of positive classes in test set\n",
    "test_set_positives=len(test_set[test_set['class']=='surveil'])\n",
    "test_set = test_set.drop(['class'], axis=1)\n",
    "\n",
    "#convert to array and normalise\n",
    "train_set = preprocessing.MinMaxScaler().fit_transform(train_set.values)\n",
    "test_set = preprocessing.MinMaxScaler().fit_transform(test_set.values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying A Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim was to find an algorithm which would identify outliers in the data. For this dataset, aircraft which behave oddly may well be surveillance aircraft. I used an autoencoder neural network to do this, here's how they work:\n",
    "\n",
    "I'm not going to go into too much detail about autoencoders. In a sentance, they take the input data, compress it, and then try and 're-predict' the input data from the compressed version. Here's how you'd represent one, with nodes and links:\n",
    "\n",
    "![alt text](autoencoder_image.png \"Title\")\n",
    "\n",
    "in the example above, a noisy image forms the input data. By compressing it and then recreating it, the noise is removed, giving a clearer version of the original image. \n",
    "At first glance this seems weird: training something to recreate the input data? This becomes useful when we consider what happens if we feed the network an unusual data point. By training it on typical data points, it learns to approximat to something which is typical. However, if the trained network is then given an anomolous point, it is likely to recreate it with a high degree of error. By measuring the difference (ie error) between the input data and the recreated points, we can identify which are the largest outliers in the data.\n",
    "\n",
    "In other words, a properly trained autoencoder can spot unusual data points.\n",
    "\n",
    "To do this with Keras/Tensorflow we first define the layers of the network. The input and output layers must be the same size as the data, with a node for each attribute. The intermediate layers wre half the size, to allow the network to compress the records as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define layers\n",
    "input_dim = test_set.shape[1]\n",
    "encoding_dim = int(input_dim/2)\n",
    "\n",
    "input_layer = Input(shape=(input_dim, ))\n",
    "encoder = Dense(encoding_dim, activation=\"tanh\", \n",
    "                activity_regularizer=regularizers.l1(10e-5))(input_layer)\n",
    "encoder = Dense(int(encoding_dim / 2), activation=\"relu\")(encoder)\n",
    "decoder = Dense(int(encoding_dim / 2), activation='tanh')(encoder)\n",
    "decoder = Dense(input_dim, activation='relu')(decoder)\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next the model was run and the best performing model saved. Note, all the parameters like the optimiser and the loss function. These can be changes along with the number and size of the layers. This process is called hyperparameter tuning and we seek to find the best combination of hyperparameters to obtain the best results. For this excercise, some random paramaters were tried and the best selection chosen. Normally I'd tune the parameters far more extensivley but since this is just a demo I didn't want to get too bogged down with this part.\n",
    "\n",
    "Note, the model was only run on the 10% of aircraft which were used as the training set. Ideally I'd train it only on aircraft which were known not to be spyplanes. This would make it optimised for reconstructing 'normal' aircraft patterns only and unusual aircraft not seen in the training set would be reconstructed with a high degree of error. I figured very few of the training set aircraft would br spyplanes so this is probably not a huge issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "nb_epoch = 100\n",
    "batch_size = 50\n",
    "autoencoder.compile(optimizer='Adamax', \n",
    "                    loss='mean_squared_error', \n",
    "                    metrics=['accuracy'])\n",
    "checkpointer = ModelCheckpoint(filepath=\"model.h5\",\n",
    "                               verbose=0,\n",
    "                               save_best_only=True)\n",
    "tensorboard = TensorBoard(log_dir='./logs',\n",
    "                          histogram_freq=0,\n",
    "                          write_graph=True,\n",
    "                          write_images=True)\n",
    "history = autoencoder.fit(train_set,train_set,\n",
    "                    epochs=nb_epoch,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(test_set,test_set),\n",
    "                    verbose=1,\n",
    "                    callbacks=[checkpointer, tensorboard]).history\n",
    "\n",
    "autoencoder=load_model('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to test the model on the dataset of 'known' aircraft. Remember, I am not convinced that all of those labelled 'spyplanes' are indeed spyplanes. Similarly, many of the aircraft labelled 'normal' could well be spylplanes. I was therfore not expecting great performance. To classify aircraft, I ensured the model only labelled the top 97 anomalous entries as spyplanes, because that is how many were labelled as such in the original 'known' aircraft dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#predict on testing set\n",
    "predictions=autoencoder.predict(test_set)\n",
    "rmse = pow(np.mean(np.power(test_set - predictions, 2), axis=1),0.5)\n",
    "error_table = pd.DataFrame({'reconstruction_error': rmse,\n",
    "                        'actual_class': test_set_labels})\n",
    "#we know how many entries have a positive in the test set. Take this number and label the predictions with the highest rmse (ie the outliers) with the positiveclass prediction\n",
    "error_table['predicted_class'] = np.where(error_table['reconstruction_error'] >= min(error_table.nlargest(int(test_set_positives),'reconstruction_error', keep='first')['reconstruction_error']), 'surveil', 'other')\n",
    "\n",
    "#get confusion matrix\n",
    "print(confusion_matrix(error_table['actual_class'],error_table['predicted_class']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I initially ran the script, 31 of the 97 spyplanes were correctly identified. By tuning the parameters, I managed to improve this to 54, which gives the following confusion matrix:\n",
    "\n",
    "|              |   normal   |  spyplane     |\n",
    "|--------------|------------|---------------|\n",
    "|   normal     |    457     |       43      |\n",
    "|--------------|------------|---------------|\n",
    "|   spyplane   |     43     |      54       |\n",
    "\n",
    "At first glance this might look like a not-too-great result but I was actually pretty impressed how well it performed - it found 54 of the 97 aircraft which were known to be government operated. Let's remember that the algorithm is identifying unusual aircraft just by the fact they fly in an irregular pattern - it is not using any known data to correlate observed aircraft with known spyplanes.\n",
    "\n",
    "Let's also remember that some of the aircraft identified as spyplanes which were labelled as 'normal' may indeed be spyplanes. The test data relies on accurate labelling of aircraft which is impossible.\n",
    "\n",
    "Let's have a look at the reconstruction error for each of the aircraft in the test set. This is a measure of how much of an outlier each aircraft was - recall the top 97 outliers were taken to be spyplanes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot error\n",
    "groups=error_table.groupby('actual_class')\n",
    "fig,ax=plt.subplots()\n",
    "for name, group in groups:\n",
    "    ax.plot(group.index,group.reconstruction_error,marker='o', ms=3.5, linestyle='',\n",
    "            label= name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](classes_scatter.png \"reconstruction error for all instances in the training set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that is cool. The plot above shows how 'wrong' the autoencoder was when predicting each aircraft. In other worse, the higher a point is on the y axis (each point is an aircraft), the more anomolous that point is. Although far from perfect, the testing examples we believe are spyplanes indeed tend to be more anomolous. This plot strongly suggests this method is capable of identifying surveillance aircraft. Remember, the plot was created without any labels on the training/testing data. So, the algorithm has no idea which examples are spyplanes...it doesn't even know this is aircraft data at all - it has just identified points in the data which are dissimilar to other points. By coloring the points with their appropriate class we have shown that the autoencoder is able to predict which aircraft are spyplanes significantly better than chance (guessing randomly would only identify 15 spyplanes correctly on average)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Applying the model to the unlabelled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a good degree of confidence that this approach can distinguish between normal aircraft and spyplanes (well, government operated aircraft anyway) it's time to apply it to the whole dataset and see which aircraft are identified as most likely to be spyplanes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](full_dataset_scatter.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The higher the x value of a point, the more likely it is to represent a spyplane. Of course, we don't know how many spyplanes are in the dataset, so we can't pink a threshold value for the error. However, just for fun, I picked 0.2 as the threshold - it'd pretty arbritrary but above that we have only the most anomalous data points. So, I'm going to go ahead and say these points represent spyplanes. Let's get the identification numbers for those aircraft:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#get identifiers for predictes spyplanes\n",
    "error_table['adshex']=df_adshex\n",
    "positive_identifications=error_table[error_table['reconstruction_error']>=0.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's 160 candidate aircraft identified. Let's see how many of the ones I identified were in common with the 101 identified by Peter Aldhous, the original author of the Buzzfeed News article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compare to previous results\n",
    "pa_results=pd.read_csv(\"pa_candidates.csv\")\n",
    "common=set.intersection(set(positive_identifications['adshex']),set(pa_results['adshex']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 17 were in common with the previous results. This is not necessarily a bad thing since the two methods used completely different approaches. Both could be good classifiers, neither could be or one could be good and the other bad (or anything between these extremes). Without further data it is impossible to know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using an unsupervised approach is the way to go when we cannot be sure whether our data is labelled correctly. For this problem, we had some labelled data but personally I wasn't very confident in its reliability. The unsupervised approach looked for 'odd' data points rather than trying to fit them to known examples. \n",
    "\n",
    "This application wasa bit of fun really, rather than a practical example. However, autoencoders are essential tools. We can imagine this approach being used for all kinds of examples such as equipment failure detection, monitoring medical patients or early warning systems.\n",
    "\n",
    "With some more tuning, I believe my approach could be improved further, I may revisit this project in the future - keep an eye on the gitHub repository if you are interested.\n",
    "\n",
    "https://github.com/Alex-Hall-Data/autoencoder-anomaly-detection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
